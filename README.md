# üöñ NYC Taxi ETL Pipeline & Analysis

## üìå Descripci√≥n del Proyecto
Este proyecto consiste en un pipeline de **Ingenier√≠a de Datos** end-to-end para procesar, limpiar y analizar datos de viajes de taxis amarillos en Nueva York (Dataset de Agosto 2025). 

El objetivo principal fue transformar datos crudos en insights de negocio valiosos, identificando patrones de rentabilidad y comportamiento de la demanda.

## üõ†Ô∏è Tecnolog√≠as y Herramientas
* **Lenguaje:** Python 3.10
* **Procesamiento Big Data:** PySpark (SparkSession, DataFrames, SparkSQL)
* **An√°lisis y Visualizaci√≥n:** Pandas, Matplotlib
* **Entorno:** Google Colab
* **Formato de Datos:** Parquet (Columnar) y CSV

## ‚öôÔ∏è Arquitectura del Pipeline (ETL)

### 1. Extract (Extracci√≥n)
* Ingesta de datos masivos desde fuentes p√∫blicas (AWS S3) en formato **Parquet**.
* Ingesta de tablas dimensionales (Zonas/Barrios) en formato **CSV**.

### 2. Transform (Transformaci√≥n)
* **Limpieza de Datos:** Filtrado de outliers (viajes de 0 minutos, tarifas negativas, distancias nulas).
* **Manejo de Fechas:** Conversi√≥n de Unix Timestamps para c√°lculo preciso de duraci√≥n de viajes.
* **Feature Engineering:** Creaci√≥n de nuevas m√©tricas de negocio:
    * `duration_minutes`: Duraci√≥n exacta del viaje.
    * `usd_per_mile`: M√©trica de rentabilidad por milla recorrida.
    * `tiempo_humano`: Formato legible para reportes.

### 3. Load (Carga)
* Enriquecimiento de datos mediante **Joins** con la tabla de zonas geogr√°ficas.
* Almacenamiento de los datos procesados ("Silver Table") en formato parquet para optimizar futuras consultas.

## üìä Insights y Resultados Clave
Tras procesar m√°s de **2.5 millones de registros**, el an√°lisis revel√≥:
* **Zonas m√°s rentables:** Los aeropuertos (JFK, LaGuardia) generan las propinas promedio m√°s altas.
* **Patrones de pago:** Se filtraron pagos en efectivo para analizar propinas reales registradas en tarjeta.
* **Calidad de datos:** Se detectaron y limpiaron inconsistencias en los registros de tiempo y distancia.

## üöÄ C√≥mo ejecutar este proyecto
Este proyecto est√° dise√±ado para correr en Google Colab o cualquier entorno local con Spark instalado.
1.  Clonar el repositorio.
2.  Abrir el archivo `.ipynb` en Jupyter o Colab.
3.  Ejecutar las celdas secuencialmente (el script descarga autom√°ticamente los datasets necesarios).

---
*Proyecto realizado por [Tu Nombre] - [A√±o]*
